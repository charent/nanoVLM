{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "5fa568f8",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import Qwen3ForCausalLM, Qwen3Config, AutoTokenizer\n",
                "from transformers.generation import GenerationConfig"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "7161b3df",
            "metadata": {},
            "outputs": [],
            "source": [
                "model_path = r'model_save\\Qwen\\Qwen3-0___6B'\n",
                "device = 'cpu'\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
                "model1 = Qwen3ForCausalLM.from_pretrained(model_path).to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "03568e27",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "您好！我是你的虚拟助手，可以为您提供帮助和支持。如果您有任何问题或需要帮助，请随时告诉我！\n"
                    ]
                }
            ],
            "source": [
                "def test_model(model: Qwen3ForCausalLM):\n",
                "    messages = [\n",
                "    {\"role\": \"user\", \"content\": '介绍一下你自己'}\n",
                "]\n",
                "    text = tokenizer.apply_chat_template(\n",
                "        messages,\n",
                "        tokenize=False,\n",
                "        add_generation_prompt=True,\n",
                "        enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
                "    )\n",
                "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
                "\n",
                "    generated_ids = model.generate(\n",
                "        **model_inputs,\n",
                "        generation_config=GenerationConfig(do_sample=False, max_new_tokens=32),\n",
                "        use_model_defaults=False\n",
                "    )\n",
                "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
                "\n",
                "    # the result will begin with thinking content in <think></think> tags, followed by the actual response\n",
                "    print(tokenizer.decode(output_ids, skip_special_tokens=True))\n",
                "test_model(model1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ae726662",
            "metadata": {},
            "outputs": [],
            "source": [
                "# config = Qwen3Config.from_pretrained('Qwen3-0___6B')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aec24fd7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import re\n",
                "# for k, v in config.__dict__.items():\n",
                "#     if isinstance(v, int|float|str|bool):\n",
                "#         t = re.findall(r'\\'(.*)\\'', str(type(v)))[0]\n",
                "#         print(f\"{k}: {t} = {v}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "7aa97536",
            "metadata": {},
            "outputs": [],
            "source": [
                "from dataclasses import dataclass\n",
                "\n",
                "@dataclass\n",
                "class VLMCOFIG:\n",
                "    vocab_size: int = 151936\n",
                "    max_position_embeddings: int = 40960\n",
                "    hidden_size: int = 1024\n",
                "    intermediate_size: int = 3072\n",
                "    num_hidden_layers: int = 28\n",
                "    num_attention_heads: int = 16\n",
                "    use_sliding_window: bool = False\n",
                "    max_window_layers: int = 28\n",
                "    num_key_value_heads: int = 8\n",
                "    head_dim: int = 128\n",
                "    hidden_act: str = 'silu'\n",
                "    initializer_range: float = 0.02\n",
                "    rms_norm_eps: float = 1e-06\n",
                "    use_cache: bool = True\n",
                "    rope_theta: int = 1000000\n",
                "    attention_bias: bool = False\n",
                "    attention_dropout: float = 0.0\n",
                "    tie_word_embeddings: bool = True\n",
                "    chunk_size_feed_forward: int = 0\n",
                "    is_encoder_decoder: bool = False\n",
                "    is_decoder: bool = False\n",
                "    add_cross_attention: bool = False\n",
                "    tie_encoder_decoder: bool = False\n",
                "    bos_token_id: int = 151643\n",
                "    eos_token_id: int = 151645\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "8db6bfc1",
            "metadata": {},
            "outputs": [],
            "source": [
                "new_config = Qwen3Config(**VLMCOFIG().__dict__)\n",
                "\n",
                "new_model = Qwen3ForCausalLM(new_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "d373a3d6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "_countries_countries_countries_countries_countries_countries_countries_countries_countries_countries_countries_countries_countries_countries_countries_countries_countries_countries_countries_countries_countries_countries_countries都知道عمارعمارعمارعمارعمارعمارعمارعمار\n"
                    ]
                }
            ],
            "source": [
                "test_model(new_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "919145e4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "您好！我是你的虚拟助手，可以为您提供帮助和支持。如果您有任何问题或需要帮助，请随时告诉我！\n"
                    ]
                }
            ],
            "source": [
                "new_model.load_state_dict(model1.state_dict())\n",
                "test_model(new_model)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "4aef0f36",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import SiglipVisionModel, AutoModel, SiglipTextModel"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "bc1a5f79",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "88.625244140625"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "visionModel = SiglipVisionModel.from_pretrained(r'model_save\\google\\siglip2-base-patch16-256')\n",
                "visionModel.num_parameters() / (1024**2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b57a5ab1",
            "metadata": {},
            "outputs": [],
            "source": [
                "visionModel.config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8ea877aa",
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "for k, v in visionModel.config.__dict__.items():\n",
                "    if isinstance(v, int|float|str|bool):\n",
                "        t = re.findall(r'\\'(.*)\\'', str(type(v)))[0]\n",
                "        if isinstance(v, str):\n",
                "            print(f\"{k}: {t} = '{v}'\")\n",
                "        else:\n",
                "            print(f\"{k}: {t} = {v}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "dd8cbdd2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# model = SiglipTextModel.from_pretrained(r'model_save\\google\\siglip2-base-patch16-256')\n",
                "# model.num_parameters() / (1024**2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "862f7e67",
            "metadata": {},
            "outputs": [],
            "source": [
                "# model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "c73e5309",
            "metadata": {},
            "outputs": [],
            "source": [
                "from models.config import VLMConfig\n",
                "from models.vision_language_model import VisionLanguageModel\n",
                "from models.processors import get_tokenizer, get_image_processor, vlm_input_apply_chat_template\n",
                "import torch\n",
                "from PIL import Image\n",
                "import os\n",
                "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
                "torch.set_default_dtype(torch.bfloat16)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "c1f77e4c",
            "metadata": {},
            "outputs": [],
            "source": [
                "config = VLMConfig()\n",
                "device = 'cuda:0'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "51eb608c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading weights from pretrained\n"
                    ]
                }
            ],
            "source": [
                "tokenizer = get_tokenizer(config.lm_pretrain_path, config.vlm_extra_tokens, config.lm_chat_template)\n",
                "image_processor = get_image_processor(256)\n",
                "model = VisionLanguageModel(config, load_from_pretrained=True).to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "891c407d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[151644, 872, 198, 53481, 100158, 108893, 45930, 151652, 151655, 151653, 151645, 198, 151644, 77091, 198]\n"
                    ]
                }
            ],
            "source": [
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "             {\"type\": \"text\", \"text\": \"描述一下这张图片\"},\n",
                "            {\n",
                "                \"type\": \"image\",\n",
                "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
                "            },\n",
                "           \n",
                "        ],\n",
                "    }\n",
                "]\n",
                "encoded_prompt = tokenizer.apply_chat_template(\n",
                "    messages, tokenize=True, add_generation_prompt=True\n",
                ")\n",
                "tokens = torch.tensor(encoded_prompt).to(device)\n",
                "print(encoded_prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "842f5a70",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<|im_start|>user\n",
                        "描述一下这张图片<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
                        "<|im_start|>assistant\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "print(tokenizer.apply_chat_template(\n",
                "    messages, tokenize=False, add_generation_prompt=True\n",
                "))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "265218f8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# from transformers import Qwen2_5_VLProcessor\n",
                "# from qwen_vl_utils import process_vision_info\n",
                "# ps = Qwen2_5_VLProcessor.from_pretrained('model_save/qwen25_vl_ps')\n",
                "\n",
                "# text = ps.apply_chat_template(\n",
                "#     messages, tokenize=False, add_generation_prompt=True\n",
                "# )\n",
                "# print(text)\n",
                "# image_inputs, video_inputs = process_vision_info(messages)\n",
                "# print(image_inputs, type(image_inputs[0]))\n",
                "# inputs = ps(\n",
                "#     text=[text],\n",
                "#     images=image_inputs,\n",
                "#     videos=video_inputs,\n",
                "#     padding=True,\n",
                "#     return_tensors=\"pt\",\n",
                "# )\n",
                "# print('\\n')\n",
                "# print(ps.decode(inputs['input_ids'][0]))\n",
                "# inputs['input_ids']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cd9d5ee5",
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = vlm_input_apply_chat_template(tokenizer, messages,).to(device)\n",
                "\n",
                "img = Image.open(r'assets/image.png').convert(\"RGB\")\n",
                "img_t = image_processor(img).unsqueeze(0).to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "89535170",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  >> Generation 1: 1. **图片的描述**：  \n",
                        "图片显示了一个清晰、直观的描述，能够帮助\n",
                        "  >> Generation 2: 1. 请描述这张图片的内容。\n",
                        "  >> Generation 3: 请描述一下这张图片的内容。\n",
                        "  >> Generation 4: 1. **图片中的场景**：  \n",
                        "- 一个宁静的公园，湖面平静，\n",
                        "  >> Generation 5: 1. **图片描述**  \n",
                        "1. **图片描述**  \n",
                        "1. **图片描述**\n"
                    ]
                }
            ],
            "source": [
                "for i in range(5):\n",
                "    gen = model.generate(inputs['input_ids'], img_t, max_new_tokens=20)\n",
                "    out = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n",
                "    print(f\"  >> Generation {i+1}: {out}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "32039ec1",
            "metadata": {},
            "outputs": [],
            "source": [
                "save_path = 'model_save/my_vlm_model'\n",
                "model.save_pretrained(save_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "89503161",
            "metadata": {},
            "outputs": [],
            "source": [
                "from models.vision_language_model import VisionLanguageModel\n",
                "save_path = 'model_save/my_model'\n",
                "model2 = VisionLanguageModel.from_pretrained(save_path).to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "dcb53258",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "681.062744140625"
                        ]
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model2.num_parameters() / 1024**2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "a9bc72a1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        ">> Generation 1: 好的，以下是关于您这张图片的描述：\n",
                        "\n",
                        "* 该图片展示了一幅关于现代中国\n",
                        ">> Generation 2: 好的，我来描述一下这张图片。图片中有一张红色的圆点，大小为\n",
                        ">> Generation 3: 好的，我来帮你描述这张图片。如果你是在寻找一些关于这个图片的描述，我可以\n",
                        ">> Generation 4: 好的，以下是针对您上传图片的描述：\n",
                        "\n",
                        "我注意到您上传了图片，可能与您\n",
                        ">> Generation 5: 好的，我现在会根据你的要求来描述这张图片。如果你愿意的话，可以告诉我图片中的\n"
                    ]
                }
            ],
            "source": [
                "with torch.no_grad():\n",
                "    for i in range(5):\n",
                "        gen = model2.generate(inputs['input_ids'], img_t, max_new_tokens=20)\n",
                "        out = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n",
                "        print(f\">> Generation {i+1}: {out}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4a82e3b1",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "py312",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
